# encoder related
encoder: sanm_chunk_opt
encoder_conf:
    output_size: 320    # dimension of attention
    attention_heads: 4
    linear_units: 1280  # the number of units of position-wise feed forward
    num_blocks: 40      # the number of encoder blocks
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.1
    input_layer: pe # encoder architecture type
    pos_enc_class: SinusoidalPositionEncoder
    normalize_before: true
    kernel_size: 11
    sanm_shfit: 0
    selfattention_layer_type: sanm
    chunk_size:
      - 20
      - 60
    stride:
      - 10
      - 40
    pad_left:
      - 5
      - 10
    encoder_att_look_back_factor:
      - 0
      - 0
    decoder_att_look_back_factor:
      - 0
      - 0

# encoder related
encoder2: sanm_chunk_opt
encoder2_conf:
    output_size: 320    # dimension of attention
    attention_heads: 4
    linear_units: 1280  # the number of units of position-wise feed forward
    num_blocks: 20      # the number of encoder blocks
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.1
    input_layer: pe # encoder architecture type
    pos_enc_class: SinusoidalPositionEncoder
    normalize_before: true
    kernel_size: 21
    sanm_shfit: 0
    selfattention_layer_type: sanm
    chunk_size:
      - 45
      - 70
    stride:
      - 35
      - 50
    pad_left:
      - 5
      - 10
    encoder_att_look_back_factor:
      - 0
      - 0
    decoder_att_look_back_factor:
      - 0
      - 0

# decoder related
decoder: fsmn_scama_opt
decoder_conf:
    attention_dim: 256
    attention_heads: 4
    linear_units: 1024
    num_blocks: 12
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    self_attention_dropout_rate: 0.1
    src_attention_dropout_rate: 0.1
    att_layer_num: 6
    kernel_size: 11
    concat_embeds: true

# decoder related
decoder2: fsmn_scama_opt
decoder2_conf:
    attention_dim: 320
    attention_heads: 4
    linear_units: 1280
    num_blocks: 12
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    self_attention_dropout_rate: 0.1
    src_attention_dropout_rate: 0.1
    att_layer_num: 6
    kernel_size: 11
    concat_embeds: true

stride_conv: stride_conv1d
stride_conv_conf:
    kernel_size: 2
    stride: 2
    pad:
      - 0
      - 1

predictor: cif_predictor_v2
predictor_conf:
    idim: 320
    threshold: 1.0
    l_order: 1
    r_order: 1

predictor2: cif_predictor_v2
predictor2_conf:
    idim: 320
    threshold: 1.0
    l_order: 1
    r_order: 1

# hybrid CTC/attention
model: uniasr
model_conf:
    ctc_weight: 0.0
    lsm_weight: 0.1     # label smoothing option
    length_normalized_loss: true
    predictor_weight: 1.0
    decoder_attention_chunk_type: chunk
    ctc_weight2: 0.0
    predictor_weight2: 1.0
    decoder_attention_chunk_type2: chunk
    loss_weight_model1: 0.5
    enable_maas_finetune: true

# minibatch related
batch_type: length
batch_bins: 2000
num_workers: 16

dataset_conf:
    filter_conf:
        min_length: 10
        max_length: 250
        min_token_length: 1
        max_token_length: 200
    shuffle: True
    shuffle_conf:
        shuffle_size: 10240
        sort_size: 500
    batch_conf:
        batch_type: token
        batch_size: 2000
    num_workers: 16

# optimization related
accum_grad: 1
grad_clip: 5
max_epoch: 20
val_scheduler_criterion:
    - valid
    - acc
best_model_criterion:
-   - valid
    - acc
    - max
keep_nbest_models: 20

optim: adam
optim_conf:
   lr: 0.0001
scheduler: warmuplr
scheduler_conf:
   warmup_steps: 30000

specaug: specaug_lfr
specaug_conf:
    apply_time_warp: false
    time_warp_window: 5
    time_warp_mode: bicubic
    apply_freq_mask: true
    freq_mask_width_range:
    - 0
    - 30
    lfr_rate: 6
    num_freq_mask: 1
    apply_time_mask: true
    time_mask_width_range:
    - 0
    - 12
    num_time_mask: 1


log_interval: 50
normalize: None
split_with_space: true
unused_parameters: true
